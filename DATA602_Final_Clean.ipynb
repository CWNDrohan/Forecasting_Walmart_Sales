{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed05194d",
      "metadata": {
        "id": "ed05194d"
      },
      "source": [
        "### <p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Introduction</p>\n",
        "\n",
        " #### In Retail Industry and chain of stores one of the biggest issue they face are supply chain management. The component of supply chain management (SCM) involved with determining how best to fulfill the requirements created from the Demand Plan.\n",
        "\n",
        " *It's objective is to balance supply and demand in a manner that achieves the financial and service objectives of the enterprise.*\n",
        "\n",
        "  If we look into the case of a retail chain stores one of the basic case is to know the demand of products that are sold in the store. If the decision making authority know whats the demand of each products for a week or month, they would be able to plan the supply chain accordingly. If that is possible this would save a lot of money for them because they don't have to overstock or can plan their Logistics accordingly.\n",
        "\n",
        "\n",
        "### <p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Data</p>\n",
        "\n",
        "### There are 3 Datasets :\n",
        "\n",
        "#### Stores:\n",
        "- Store: The store number. Range from 1–45.\n",
        "- Type: Three types of stores ‘A’, ‘B’ or ‘C’.\n",
        "- Size: Sets the size of a Store would be calculated by the no. of products available in the particular store ranging from 34,000 to 210,000.\n",
        "\n",
        "***primary key is Store***\n",
        "\n",
        "#### Sales:\n",
        "    -Date: The date of the week where this observation was taken.\n",
        "    -Weekly_Sales: The sales recorded during that Week.\n",
        "    -Store: The store which observation in recorded 1–45\n",
        "    -Dept: One of 1–99 that shows the department.\n",
        "    -IsHoliday: Boolean value representing a holiday week or not.\n",
        "\n",
        "***primary key is a combination of (Store,Dept,Date).***\n",
        "\n",
        "#### Features:\n",
        "\n",
        "    -Temperature: Temperature of the region during that week.\n",
        "    -Fuel_Price: Fuel Price in that region during that week.\n",
        "    -MarkDown1:5 : Represents the Type of markdown and what quantity was available during that week.\n",
        "    -CPI: Consumer Price Index during that week.\n",
        "    -Unemployment: The unemployment rate during that week in the region of the store.\n",
        "\n",
        "***primary key here is a combination of (Store,Date)***\n",
        "\n",
        "![](https://i.imgur.com/XuDXqGU.png)\n",
        "\n",
        "\n",
        "### <p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Plan of Action</p>\n",
        "\n",
        "    1. We will perform dertailed EDA & employ several algortihms we learned in class\n",
        "    2. Next we will build the following models to predict future sales.\n",
        "\n",
        "\n",
        " **List of Models -**\n",
        "\n",
        "     1. XXXXXXXXXX -- Please fill in\n",
        "     2. YYYYYYYYYY -- Please fill in\n",
        "     3. ZZZZZZZZZZ -- Please fill in\n",
        "     4. AAAAAAAAAA -- Please fill in\n",
        "     5. Gradient Booster\n",
        "     6. Extreme Gradient Boooster\n",
        "     7. Fully Connected NN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07fc437e",
      "metadata": {
        "id": "07fc437e"
      },
      "outputs": [],
      "source": [
        "# importing basic packages\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O\n",
        "import datetime # manipulating date formats\n",
        "# Viz\n",
        "import matplotlib.pyplot as plt # basic plotting\n",
        "import seaborn as sns # for plots\n",
        "import mlxtend\n",
        "%matplotlib inline\n",
        "\n",
        "# settings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678446e5",
      "metadata": {
        "id": "678446e5"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Dataset Importing and Querying</p>\n",
        "\n",
        "    We will load all datasets and merge them into one dataset that gives whole data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da81b22",
      "metadata": {
        "id": "5da81b22"
      },
      "outputs": [],
      "source": [
        "# Reading the data using pandas dataframe\n",
        "features = pd.read_csv('/Users/cwndroha/Downloads/walmart-recruiting-store-sales-forecasting/features/features.csv')\n",
        "train    = pd.read_csv('/Users/cwndroha/Downloads/walmart-recruiting-store-sales-forecasting/train/train.csv')\n",
        "stores   = pd.read_csv('/Users/cwndroha/Downloads/walmart-recruiting-store-sales-forecasting/stores.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc6c01f",
      "metadata": {
        "id": "3dc6c01f"
      },
      "source": [
        "- Printing sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfb790c",
      "metadata": {
        "id": "fdfb790c"
      },
      "outputs": [],
      "source": [
        "df_names=['features','stores','train']\n",
        "df_list=[features,stores,train]\n",
        "for i in range(3):\n",
        "    print('--'*25)\n",
        "    print(f'Dataframe {df_names[i]} has {df_list[i].shape[0]} rows and {df_list[i].shape[1]} columns.')\n",
        "    print('--'*25)\n",
        "    display(df_list[i].head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411f3142",
      "metadata": {
        "id": "411f3142"
      },
      "source": [
        "- Merging Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c08b0b",
      "metadata": {
        "id": "d3c08b0b"
      },
      "outputs": [],
      "source": [
        "df = train.merge(stores, how='left').merge(features, how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d4a7f9",
      "metadata": {
        "id": "68d4a7f9"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Data Cleaning</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ba28c0",
      "metadata": {
        "id": "70ba28c0"
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = pd.to_datetime(df['Date']).dt.year\n",
        "df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
        "df['Week'] = pd.to_datetime(df['Date']).dt.week\n",
        "df['Day'] = pd.to_datetime(df['Date']).dt.day\n",
        "df.replace({'A': 1, 'B': 2,'C':3},inplace=True)\n",
        "df['IsHoliday'] = df['IsHoliday'].apply(lambda x: 1 if x == True else 0)\n",
        "df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397fe14d",
      "metadata": {
        "id": "397fe14d"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9417d7",
      "metadata": {
        "id": "2f9417d7"
      },
      "outputs": [],
      "source": [
        "# Want to check the date column is in object format or datetime\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751ce279",
      "metadata": {
        "id": "751ce279"
      },
      "source": [
        "- Calculating missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4b7d27",
      "metadata": {
        "id": "be4b7d27"
      },
      "outputs": [],
      "source": [
        "print('Percentages of missing values in merged dataframe.')\n",
        "(100*df.isna().sum()/df.shape[0]).sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64755959",
      "metadata": {
        "id": "64755959"
      },
      "outputs": [],
      "source": [
        "# Imputing the null values in markdown columns with 0\n",
        "df.iloc[:,9:14]=df.iloc[:,9:14].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07225419",
      "metadata": {
        "id": "07225419"
      },
      "outputs": [],
      "source": [
        "print('Percentages of missing values in merged dataframe.')\n",
        "(100*df.isna().sum()/df.shape[0]).sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e2ac257",
      "metadata": {
        "id": "9e2ac257"
      },
      "source": [
        "- Creating Markdown variable which is equal to 1 if any markdown is equal to 1, 0 otherwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c69de0d",
      "metadata": {
        "id": "1c69de0d"
      },
      "outputs": [],
      "source": [
        "df['markdown'] = df.iloc[:,9:14].sum(axis=1)\n",
        "df['markdown'] = df['markdown'].apply(lambda x:0 if x==0 else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae6bea9",
      "metadata": {
        "id": "8ae6bea9"
      },
      "source": [
        "- Moving Target Variable to End of DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e257792",
      "metadata": {
        "id": "8e257792"
      },
      "outputs": [],
      "source": [
        "cols = list(df.columns) # List of colmuns of dataframe df\n",
        "cols.pop(cols.index('Weekly_Sales')) # Moving Target Variable to end of DF to better read Corr Matrix\n",
        "df = df[cols+['Weekly_Sales']] # Moving Target Variable to end of DF to better read Corr Matrix\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5861359d",
      "metadata": {
        "id": "5861359d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(df.corr(), cmap='summer', annot=True)\n",
        "plt.title('Correlation Matrix -- feature importance', fontsize=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d2c704",
      "metadata": {
        "id": "16d2c704"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr # For Spearman correlation\n",
        "\n",
        "cols=df.columns\n",
        "cols=cols.drop(['Date'])\n",
        "\n",
        "for col in cols[:-1]:\n",
        "    rho, p = spearmanr(df[col].values, df['Weekly_Sales'].values)\n",
        "    print('Spearman correlation between Weekly_Sales and %s is %s' %(col, round(rho,4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f4538dd",
      "metadata": {
        "id": "5f4538dd"
      },
      "outputs": [],
      "source": [
        "cr = df.corr()\n",
        "plt.figure(figsize=(10,10), dpi=80)\n",
        "plt.title('Multicollinearity', fontsize=30)\n",
        "sns.heatmap(cr[(cr>=0.6)|(cr<=-0.6)],annot=True,cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefde4b2",
      "metadata": {
        "id": "eefde4b2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "df.groupby('Date')['Weekly_Sales'].mean().plot()\n",
        "plt.grid()\n",
        "plt.title('Average weekly Sales of the company across all stores in given timeframe', fontsize=18)\n",
        "plt.ylabel('Sales', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753b0ebe",
      "metadata": {
        "id": "753b0ebe"
      },
      "outputs": [],
      "source": [
        "weekly_sales = df.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean', 'median']})\n",
        "weekly_sales2010 = df.loc[df['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\n",
        "weekly_sales2011 = df.loc[df['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\n",
        "weekly_sales2012 = df.loc[df['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values)\n",
        "sns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values)\n",
        "sns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values)\n",
        "\n",
        "plt.grid()\n",
        "plt.title('Average weekly Sales by Year.', fontsize=18)\n",
        "plt.xticks(np.arange(1, 53, step=1))\n",
        "plt.legend(['2010', '2011', '2012'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b969326",
      "metadata": {
        "id": "6b969326"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "df[df['Type']==1].groupby('Week').mean()['Weekly_Sales'].plot()\n",
        "df[df['Type']==2].groupby('Week').mean()['Weekly_Sales'].plot()\n",
        "df[df['Type']==3].groupby('Week').mean()['Weekly_Sales'].plot()\n",
        "plt.grid()\n",
        "plt.title('Average weekly Sales of the company by type of the store.', fontsize=18)\n",
        "plt.legend(['Type A', 'Type B', 'Type C'], loc='best', fontsize=16)\n",
        "plt.ylabel('Sales', fontsize=16)\n",
        "plt.xlabel('Weeks', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75887338",
      "metadata": {
        "id": "75887338"
      },
      "outputs": [],
      "source": [
        "def av_sales_plotter(str):\n",
        "    plt.figure(figsize=(20,5))\n",
        "    df.groupby(str).mean()['Weekly_Sales'].sort_values().plot(kind='bar',color='crimson')\n",
        "    plt.title(f'Sales of each {str}.', fontsize=18)\n",
        "    plt.ylabel('Sales', fontsize=16)\n",
        "    plt.xlabel(str, fontsize=16)\n",
        "    plt.tick_params(axis='x', labelsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0405b61",
      "metadata": {
        "id": "b0405b61"
      },
      "outputs": [],
      "source": [
        "av_sales_plotter('Dept')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1263881",
      "metadata": {
        "id": "d1263881"
      },
      "outputs": [],
      "source": [
        "av_sales_plotter('Store')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b29025",
      "metadata": {
        "id": "61b29025"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "sns.barplot(x='Store',y='Size',data=stores,order=stores.sort_values('Size')['Store'].tolist())\n",
        "plt.title('Sizes of all the stores.',fontsize=15)\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6236383",
      "metadata": {
        "id": "c6236383"
      },
      "outputs": [],
      "source": [
        "def scatter(dataset, column):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.scatter(df[column] , df['Weekly_Sales'], color = 'crimson')\n",
        "    plt.title('Scatter Plot of Actual Sales.', fontsize=15)\n",
        "    plt.ylabel('Weekly Sales')\n",
        "    plt.xlabel(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c07320",
      "metadata": {
        "id": "15c07320"
      },
      "outputs": [],
      "source": [
        "scatter(df, 'Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38019d27",
      "metadata": {
        "id": "38019d27"
      },
      "outputs": [],
      "source": [
        "# importing relevant libraries\n",
        "\n",
        "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "\n",
        "# Importing basic tensorflow modules\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import torch\n",
        "\n",
        "# Importing 'xgboost'\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79a2a1c",
      "metadata": {
        "id": "e79a2a1c"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Gradient Boosting Regressor - I.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9388b5a",
      "metadata": {
        "id": "a9388b5a"
      },
      "outputs": [],
      "source": [
        "df1=df.filter(['Store','Dept','IsHoliday','Markdown','Size','Week','Month','Weekly_Sales'])\n",
        "\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df1.columns[:-1]\n",
        "target = df1.columns[-1]\n",
        "\n",
        "X = df1[features].values\n",
        "y = df1[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a16974b0",
      "metadata": {
        "id": "a16974b0"
      },
      "outputs": [],
      "source": [
        "# Importing 'GradientBoostingRegressor'\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_reg = GradientBoostingRegressor() # Instantiating GradientBoostingRegressor\n",
        "gb_reg.fit(X_train, y_train) # Fitting the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74793a52",
      "metadata": {
        "id": "74793a52"
      },
      "outputs": [],
      "source": [
        "# Finding the predictions of gradient boosting regressor for train and test subsets\n",
        "train_y_pred = gb_reg.predict(X_train)\n",
        "test_y_pred = gb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c25f16",
      "metadata": {
        "id": "83c25f16"
      },
      "outputs": [],
      "source": [
        "r2_train_score = gb_reg.score(X_train, y_train) # Calculating R^2 score for train\n",
        "r2_test_score = gb_reg.score(X_test, y_test) # Calculating R^2 score for test\n",
        "print('R^2 score for train dataset = ', round(r2_train_score, 4), '\\n')\n",
        "print('R^2 score for test dataset = ', round(r2_test_score, 4), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b459f0c",
      "metadata": {
        "id": "2b459f0c"
      },
      "outputs": [],
      "source": [
        "# Calculating the feature importance\n",
        "\n",
        "feature_importance = gb_reg.feature_importances_\n",
        "feature_importance = 100.0*(feature_importance/np.sum(feature_importance))\n",
        "# Displaying the relative feature importance by a horizontal bar chart\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos=np.arange(sorted_idx.shape[0])+0.5\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"GB_REG -- Feature Importance w/o Hyperparameters\")\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.barh(pos, feature_importance[sorted_idx], color='crimson', align=\"center\")\n",
        "plt.yticks(pos, df1.columns[:-1][sorted_idx])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a3b9152",
      "metadata": {
        "id": "4a3b9152"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Gradient Boosting Regressor - II. Intro Hyperparameters</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e392f727",
      "metadata": {
        "id": "e392f727"
      },
      "outputs": [],
      "source": [
        "# Setting new hyperparameters for gradient boosting regressor\n",
        "gb_params = {'n_estimators': 1000, 'max_depth': 6, 'min_samples_split': 15, 'learning_rate': 0.01}\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(**gb_params) # Applying the new hyperparameters\n",
        "gb_reg.fit(X_train, y_train) # Fitting the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64174512",
      "metadata": {
        "id": "64174512"
      },
      "outputs": [],
      "source": [
        "# Finding the predictions of gradient boosting regressor for train and test subsets\n",
        "train_y_pred = gb_reg.predict(X_train)\n",
        "test_y_pred = gb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6f277d",
      "metadata": {
        "id": "ed6f277d"
      },
      "outputs": [],
      "source": [
        "r2_train_score = gb_reg.score(X_train, y_train) # Calculating R^2 score for train\n",
        "r2_test_score = gb_reg.score(X_test, y_test) # Calculating R^2 score for test\n",
        "print('R^2 score for train dataset = ', round(r2_train_score, 4), '\\n')\n",
        "print('R^2 score for test dataset = ', round(r2_test_score, 4), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0ac20f",
      "metadata": {
        "id": "bd0ac20f"
      },
      "outputs": [],
      "source": [
        "# Calculating the feature importance\n",
        "\n",
        "feature_importance = gb_reg.feature_importances_\n",
        "feature_importance = 100.0*(feature_importance/np.sum(feature_importance))\n",
        "# Displaying the relative feature importance by a horizontal bar chart\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos=np.arange(sorted_idx.shape[0])+0.5\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"GB_REG -- Feature Importance w/ Hyperparameters\")\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.barh(pos, feature_importance[sorted_idx], color='crimson', align=\"center\")\n",
        "plt.yticks(pos, df1.columns[:-1][sorted_idx])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c69980f1",
      "metadata": {
        "id": "c69980f1"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Gradient Boosting Regressor - III. Tuned Hyperparameters, Low end</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde418d8",
      "metadata": {
        "id": "bde418d8"
      },
      "outputs": [],
      "source": [
        "# Importing 'GridSearchCV' from 'sklearn'\n",
        "\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Specifying the hyperparameters\n",
        "\n",
        "#param_grid = {'n_estimators': [500,1000],\n",
        "#               'max_depth': [5,10],\n",
        "#               'min_samples_split': [10,15]}\n",
        "\n",
        "#grid = GridSearchCV(GradientBoostingRegressor(), param_grid, n_jobs=-1, verbose=3)\n",
        "\n",
        "#grid.fit(X_train,y_train)\n",
        "\n",
        "#print('\\n Best choices for hyperparameters:\\n', grid.best_params_) # Presenting best choice\n",
        "\n",
        "#grid_predictions = grid.predict(X_test) # Compute the predictions of SVM with best choice of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a904e9b2",
      "metadata": {
        "id": "a904e9b2"
      },
      "outputs": [],
      "source": [
        "# Setting new hyperparameters for gradient boosting regressor\n",
        "gb_params = {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 15, 'learning_rate': 0.01}\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(**gb_params) # Applying the new hyperparameters\n",
        "gb_reg.fit(X_train, y_train) # Fitting the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56e5199",
      "metadata": {
        "id": "e56e5199"
      },
      "outputs": [],
      "source": [
        "# Finding the predictions of gradient boosting regressor for train and test subsets\n",
        "train_y_pred = gb_reg.predict(X_train)\n",
        "test_y_pred = gb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256ca108",
      "metadata": {
        "id": "256ca108"
      },
      "outputs": [],
      "source": [
        "r2_train_score = gb_reg.score(X_train, y_train) # Calculating R^2 score for train\n",
        "r2_test_score = gb_reg.score(X_test, y_test) # Calculating R^2 score for test\n",
        "print('R^2 score for train dataset = ', round(r2_train_score, 4), '\\n')\n",
        "print('R^2 score for test dataset = ', round(r2_test_score, 4), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726189ec",
      "metadata": {
        "id": "726189ec"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Gradient Boosting Regressor - IV. Tuned Hyperparameters, High End</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85ce5fa",
      "metadata": {
        "id": "d85ce5fa"
      },
      "outputs": [],
      "source": [
        "# Importing 'GridSearchCV' from 'sklearn'\n",
        "\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Specifying the hyperparameters\n",
        "\n",
        "#param_grid = {'n_estimators': [1000,2000],\n",
        "#               'max_depth': [10,15],\n",
        "#               'min_samples_split': [15,20]}\n",
        "\n",
        "#grid = GridSearchCV(GradientBoostingRegressor(), param_grid, n_jobs=-1, verbose=3)\n",
        "\n",
        "#grid.fit(X_train,y_train)\n",
        "\n",
        "#print('\\n Best choices for hyperparameters:\\n', grid.best_params_) # Presenting best choice\n",
        "\n",
        "#grid_predictions = grid.predict(X_test) # Compute the predictions of SVM with best choice of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca641c1",
      "metadata": {
        "id": "0ca641c1"
      },
      "outputs": [],
      "source": [
        "# Setting new hyperparameters for gradient boosting regressor\n",
        "gb_params = {'n_estimators': 1000, 'max_depth': 10, 'min_samples_split': 20, 'learning_rate': 0.01}\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(**gb_params) # Applying the new hyperparameters\n",
        "gb_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4bb1a59",
      "metadata": {
        "id": "c4bb1a59"
      },
      "outputs": [],
      "source": [
        "# Finding the predictions of gradient boosting regressor for train and test subsets\n",
        "train_y_pred = gb_reg.predict(X_train)\n",
        "test_y_pred = gb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492390d9",
      "metadata": {
        "id": "492390d9"
      },
      "outputs": [],
      "source": [
        "r2_train_score = gb_reg.score(X_train, y_train) # Calculating R^2 score for train\n",
        "r2_test_score = gb_reg.score(X_test, y_test) # Calculating R^2 score for test\n",
        "\n",
        "print('R^2 score for train dataset = ', round(r2_train_score, 4), '\\n')\n",
        "print('R^2 score for test dataset = ', round(r2_test_score, 4), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb4745c",
      "metadata": {
        "id": "7fb4745c"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Extreme Gradient Boosting Regressor - I. Feature Selection</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86a6c84",
      "metadata": {
        "id": "f86a6c84"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Size','Week','Weekly_Sales'])\n",
        "\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df2.columns[:-1]\n",
        "target = df2.columns[-1]\n",
        "\n",
        "X = df2[features].values\n",
        "y = df2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1dfe4e",
      "metadata": {
        "id": "fa1dfe4e"
      },
      "outputs": [],
      "source": [
        "# Instantiate 'XGBRegressor'\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=700, max_depth=6, eta=0.04, subsample=0.8)\n",
        "\n",
        "xgb_reg.fit(X_train, y_train) # Fitting the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d2d198",
      "metadata": {
        "id": "01d2d198"
      },
      "outputs": [],
      "source": [
        "# Finding the predictions of XGBoost regressor for train and test subsets\n",
        "train_y_pred = xgb_reg.predict(X_train)\n",
        "test_y_pred = xgb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5741c659",
      "metadata": {
        "id": "5741c659"
      },
      "outputs": [],
      "source": [
        "r2_train_score = xgb_reg.score(X_train, y_train) # Calculating R^2 score for train\n",
        "r2_test_score = xgb_reg.score(X_test, y_test) # Calculating R^2 score for test\n",
        "print('R^2 score for train dataset = ', round(r2_train_score, 4), '\\n')\n",
        "print('R^2 score for test dataset = ', round(r2_test_score, 4), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d98deb",
      "metadata": {
        "id": "d1d98deb"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Extreme Gradient Boosting Regressor - II. Hyperparameter Tuning</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14016aa",
      "metadata": {
        "id": "d14016aa"
      },
      "outputs": [],
      "source": [
        "# Specifying the hyperparameters\n",
        "\n",
        "#param_grid = {'n_estimators': [500,1000],\n",
        "               #'max_depth': [5,10,15]}\n",
        "\n",
        "#grid = GridSearchCV(XGBRegressor(), param_grid, n_jobs=-1, verbose=3)\n",
        "\n",
        "#grid.fit(X_train,y_train)\n",
        "\n",
        "#print('\\n Best choices for hyperparameters:\\n', grid.best_params_) # Presenting best choice\n",
        "\n",
        "#grid_predictions = grid.predict(X_test) # Compute the predictions of SVM with best choice of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e9a32d5",
      "metadata": {
        "id": "1e9a32d5"
      },
      "outputs": [],
      "source": [
        "# Instantiate 'XGBRegressor'\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=500, max_depth=10, eta=0.04, subsample=0.8)\n",
        "\n",
        "xgb_reg.fit(X_train, y_train) # Fitting the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4b70ae",
      "metadata": {
        "id": "bc4b70ae"
      },
      "outputs": [],
      "source": [
        "# Finding the predictions of XGBoost regressor for train and test subsets\n",
        "train_y_pred = xgb_reg.predict(X_train)\n",
        "test_y_pred = xgb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b69c9f",
      "metadata": {
        "id": "e0b69c9f"
      },
      "outputs": [],
      "source": [
        "r2_train_score = xgb_reg.score(X_train, y_train) # Calculating R^2 score for train\n",
        "r2_test_score = xgb_reg.score(X_test, y_test) # Calculating R^2 score for test\n",
        "print('R^2 score for train dataset = ', round(r2_train_score, 4), '\\n')\n",
        "print('R^2 score for test dataset = ', round(r2_test_score, 4), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e041be07",
      "metadata": {
        "id": "e041be07"
      },
      "outputs": [],
      "source": [
        "# Calculating the feature importance\n",
        "\n",
        "feature_importance = xgb_reg.feature_importances_\n",
        "feature_importance = 100.0*(feature_importance/np.sum(feature_importance))\n",
        "# Displaying the relative feature importance by a horizontal bar chart\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos=np.arange(sorted_idx.shape[0])+0.5\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"XGB_REG -- Feature Importance\")\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.barh(pos, feature_importance[sorted_idx], color='crimson', align=\"center\")\n",
        "plt.yticks(pos, df1.columns[:-1][sorted_idx])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e9c1c0",
      "metadata": {
        "id": "21e9c1c0"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Fully Connected NN - I. XXX</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8efaaa61",
      "metadata": {
        "id": "8efaaa61"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Weekly_Sales'])\n",
        "df2_1=df2[df2['Store']==20] #limit to store\n",
        "df2_2=df2_1[df2_1['Dept']==92] #limit to store and dept\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df2.columns[:-1]\n",
        "target = df2.columns[-1]\n",
        "\n",
        "X = df2[features].values\n",
        "y = df2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb67f5b",
      "metadata": {
        "id": "5cb67f5b"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Size','Week','Weekly_Sales'])\n",
        "\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df2.columns[:-1]\n",
        "target = df2.columns[-1]\n",
        "\n",
        "X = df2[features].values\n",
        "y = df2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0d6a1d",
      "metadata": {
        "id": "8e0d6a1d"
      },
      "outputs": [],
      "source": [
        "# Creating a simple function that builds a fully connected neural network\n",
        "\n",
        "def create_model(n_neurons, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_neurons, input_dim=X_train.shape[1], activation='tanh')) # Layer 1 + activation\n",
        "    model.add(Dense(n_neurons, activation='tanh'))                             # Layer 2 + activation\n",
        "    model.add(Dense(n_neurons, activation='tanh'))                             # Layer 3 + activation\n",
        "    model.add(Dense(len(y_train.shape)))                                          # Output layer\n",
        "    adam = Adam(learning_rate=learning_rate)                                      # Choice of optimizer\n",
        "    # Compiling the model by specifying the choice of loss function ('mae'), and performance metrics\n",
        "    model.compile(loss='mae', optimizer=adam, metrics=['mae', tfa.metrics.r_square.RSquare()])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f71a8a",
      "metadata": {
        "id": "23f71a8a"
      },
      "outputs": [],
      "source": [
        "# Creating a model with 64 neurons and learning rate 0.001\n",
        "\n",
        "#del model\n",
        "model = create_model(n_neurons=64, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc8748f",
      "metadata": {
        "id": "2fc8748f"
      },
      "outputs": [],
      "source": [
        "# Presenting the summary of the model\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf14aa8",
      "metadata": {
        "id": "cdf14aa8"
      },
      "outputs": [],
      "source": [
        "# Training the model for 50 epochs\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "model_history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec028bab",
      "metadata": {
        "id": "ec028bab"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['mae'], label = 'train mae')\n",
        "plt.plot(model_history.history['val_mae'], label = 'test mae')\n",
        "plt.title('History of the Loss Function of the Model')\n",
        "plt.legend(['train mae', 'test mae'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07b499c",
      "metadata": {
        "id": "c07b499c"
      },
      "outputs": [],
      "source": [
        "# Plotting R^2 score from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['r_square'], label = 'train r^2')\n",
        "plt.plot(model_history.history['val_r_square'], label = 'test r^2')\n",
        "plt.title(r'$R^2$ of the Train and Test Subsets')\n",
        "plt.legend([r'train $R^2$', r'test $R^2$'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d39b425",
      "metadata": {
        "id": "4d39b425"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Fully Connected NN - II. ReLu</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c469c4",
      "metadata": {
        "id": "a9c469c4"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Size','Week','Weekly_Sales'])\n",
        "\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df2.columns[:-1]\n",
        "target = df2.columns[-1]\n",
        "\n",
        "X = df2[features].values\n",
        "y = df2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb74527",
      "metadata": {
        "id": "2fb74527"
      },
      "outputs": [],
      "source": [
        "# Creating a simple function that builds a fully connected neural network\n",
        "\n",
        "def create_model(n_neurons, learning_rate):\n",
        "    model2 = Sequential()\n",
        "    model2.add(Dense(n_neurons, input_dim=X_train.shape[1], activation='relu')) # Layer 1 + activation\n",
        "    model2.add(Dense(n_neurons, activation='relu'))                             # Layer 2 + activation\n",
        "    model2.add(Dense(n_neurons, activation='relu'))                             # Layer 3 + activation\n",
        "    model2.add(Dense(len(y_train.shape)))                                          # Output layer\n",
        "    adam = Adam(learning_rate=learning_rate)                                      # Choice of optimizer\n",
        "    # Compiling the model by specifying the choice of loss function ('mae'), and performance metrics\n",
        "    model2.compile(loss='mae', optimizer=adam, metrics=['mae', tfa.metrics.r_square.RSquare()])\n",
        "    return model2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1cc12a",
      "metadata": {
        "id": "9b1cc12a"
      },
      "outputs": [],
      "source": [
        "# Creating a model with 64 neurons and learning rate 0.001\n",
        "\n",
        "#del model2\n",
        "model2 = create_model(n_neurons=64, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09db277c",
      "metadata": {
        "id": "09db277c"
      },
      "outputs": [],
      "source": [
        "# Presenting the summary of the model\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b22f1d0",
      "metadata": {
        "id": "5b22f1d0"
      },
      "outputs": [],
      "source": [
        "# Training the model for 25 epochs\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "model_history = model2.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8ff1c0",
      "metadata": {
        "id": "1f8ff1c0"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['mae'], label = 'train mae')\n",
        "plt.plot(model_history.history['val_mae'], label = 'test mae')\n",
        "plt.title('History of the Loss Function of the Model')\n",
        "plt.legend(['train mae', 'test mae'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84162537",
      "metadata": {
        "id": "84162537"
      },
      "outputs": [],
      "source": [
        "# Plotting R^2 score from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['r_square'], label = 'train r^2')\n",
        "plt.plot(model_history.history['val_r_square'], label = 'test r^2')\n",
        "plt.title(r'$R^2$ of the Train and Test Subsets')\n",
        "plt.legend([r'train $R^2$', r'test $R^2$'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0db2e4dc",
      "metadata": {
        "id": "0db2e4dc"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Fully Connected NN - III. TanH, 128 Neurons</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02648c77",
      "metadata": {
        "id": "02648c77"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Size','Week','Weekly_Sales'])\n",
        "\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df2.columns[:-1]\n",
        "target = df2.columns[-1]\n",
        "\n",
        "X = df2[features].values\n",
        "y = df2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77ad0e7",
      "metadata": {
        "id": "a77ad0e7"
      },
      "outputs": [],
      "source": [
        "# Creating a simple function that builds a fully connected neural network\n",
        "\n",
        "def create_model(n_neurons, learning_rate):\n",
        "    model3 = Sequential()\n",
        "    model3.add(Dense(n_neurons, input_dim=X_train.shape[1], activation='tanh')) # Layer 1 + activation\n",
        "    model3.add(Dense(n_neurons, activation='tanh'))                             # Layer 2 + activation\n",
        "    model3.add(Dense(n_neurons, activation='tanh'))                             # Layer 3 + activation\n",
        "    model3.add(Dense(len(y_train.shape)))                                          # Output layer\n",
        "    adam = Adam(learning_rate=learning_rate)                                      # Choice of optimizer\n",
        "    # Compiling the model by specifying the choice of loss function ('mae'), and performance metrics\n",
        "    model3.compile(loss='mae', optimizer=adam, metrics=['mae', tfa.metrics.r_square.RSquare()])\n",
        "    return model3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d574a1d",
      "metadata": {
        "id": "2d574a1d"
      },
      "outputs": [],
      "source": [
        "# Creating a model with 128 neurons and learning rate 0.001\n",
        "\n",
        "#del model3\n",
        "model3 = create_model(n_neurons=128, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6069565b",
      "metadata": {
        "id": "6069565b"
      },
      "outputs": [],
      "source": [
        "# Presenting the summary of the model\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a301cd0",
      "metadata": {
        "id": "7a301cd0"
      },
      "outputs": [],
      "source": [
        "# Training the model for 25 epochs\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "model_history = model3.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c919dc",
      "metadata": {
        "id": "d2c919dc"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['mae'], label = 'train mae')\n",
        "plt.plot(model_history.history['val_mae'], label = 'test mae')\n",
        "plt.title('History of the Loss Function of the Model')\n",
        "plt.legend(['train mae', 'test mae'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc96a72",
      "metadata": {
        "id": "6cc96a72"
      },
      "outputs": [],
      "source": [
        "# Plotting R^2 score from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['r_square'], label = 'train r^2')\n",
        "plt.plot(model_history.history['val_r_square'], label = 'test r^2')\n",
        "plt.title(r'$R^2$ of the Train and Test Subsets')\n",
        "plt.legend([r'train $R^2$', r'test $R^2$'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19816458",
      "metadata": {
        "id": "19816458"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Fully Connected NN - IV. TanH, 64 Neurons, Learning Rate 0.0001</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51c63a0",
      "metadata": {
        "id": "e51c63a0"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Size','Week','Weekly_Sales'])\n",
        "\n",
        "# Defining the features and the target of the model\n",
        "\n",
        "features = df2.columns[:-1]\n",
        "target = df2.columns[-1]\n",
        "\n",
        "X = df2[features].values\n",
        "y = df2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11385159",
      "metadata": {
        "id": "11385159"
      },
      "outputs": [],
      "source": [
        "# Creating a simple function that builds a fully connected neural network\n",
        "\n",
        "def create_model(n_neurons, learning_rate):\n",
        "    model4 = Sequential()\n",
        "    model4.add(Dense(n_neurons, input_dim=X_train.shape[1], activation='tanh')) # Layer 1 + activation\n",
        "    model4.add(Dense(n_neurons, activation='tanh'))                             # Layer 2 + activation\n",
        "    model4.add(Dense(n_neurons, activation='tanh'))                             # Layer 3 + activation\n",
        "    model4.add(Dense(len(y_train.shape)))                                          # Output layer\n",
        "    adam = Adam(learning_rate=learning_rate)                                      # Choice of optimizer\n",
        "    # Compiling the model by specifying the choice of loss function ('mae'), and performance metrics\n",
        "    model4.compile(loss='mae', optimizer=adam, metrics=['mae', tfa.metrics.r_square.RSquare()])\n",
        "    return model4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eeded46",
      "metadata": {
        "id": "2eeded46"
      },
      "outputs": [],
      "source": [
        "# Creating a model with 64 neurons and learning rate 0.0001\n",
        "\n",
        "#del model4\n",
        "model4 = create_model(n_neurons=64, learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84d46be",
      "metadata": {
        "id": "d84d46be"
      },
      "outputs": [],
      "source": [
        "# Presenting the summary of the model\n",
        "\n",
        "model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec0df86",
      "metadata": {
        "id": "eec0df86"
      },
      "outputs": [],
      "source": [
        "# Training the model for 25 epochs\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "model_history = model4.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf0fd76",
      "metadata": {
        "id": "fdf0fd76"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['mae'], label = 'train mae')\n",
        "plt.plot(model_history.history['val_mae'], label = 'test mae')\n",
        "plt.title('History of the Loss Function of the Model')\n",
        "plt.legend(['train mae', 'test mae'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f31a6f2",
      "metadata": {
        "id": "8f31a6f2"
      },
      "outputs": [],
      "source": [
        "# Plotting R^2 score from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['r_square'], label = 'train r^2')\n",
        "plt.plot(model_history.history['val_r_square'], label = 'test r^2')\n",
        "plt.title(r'$R^2$ of the Train and Test Subsets')\n",
        "plt.legend([r'train $R^2$', r'test $R^2$'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018c126e",
      "metadata": {
        "id": "018c126e"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Fully Connected NN - V. One Store, One Dept</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e613fde7",
      "metadata": {
        "id": "e613fde7"
      },
      "outputs": [],
      "source": [
        "av_sales_plotter('Store')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcaaa9b0",
      "metadata": {
        "id": "fcaaa9b0"
      },
      "outputs": [],
      "source": [
        "av_sales_plotter('Dept')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07309675",
      "metadata": {
        "id": "07309675"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(['Store','Dept','Size','Week','Weekly_Sales'])\n",
        "df2_1=df2[df2['Store']==20] #limit to store\n",
        "df2_2=df2_1[df2_1['Dept']==92] #limit to store and dept\n",
        "\n",
        "df2_2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "806636fd",
      "metadata": {
        "id": "806636fd"
      },
      "outputs": [],
      "source": [
        "features = df2_2.columns[:-1]\n",
        "target = df2_2.columns[-1]\n",
        "\n",
        "X = df2_2[features].values\n",
        "y = df2_2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac991cd",
      "metadata": {
        "id": "4ac991cd"
      },
      "outputs": [],
      "source": [
        "# Creating a simple function that builds a fully connected neural network\n",
        "\n",
        "def create_model(n_neurons, learning_rate):\n",
        "    model5 = Sequential()\n",
        "    model5.add(Dense(n_neurons, input_dim=X_train.shape[1], activation='tanh')) # Layer 1 + activation\n",
        "    model5.add(Dense(n_neurons, activation='tanh'))                             # Layer 2 + activation\n",
        "    model5.add(Dense(n_neurons, activation='tanh'))                             # Layer 3 + activation\n",
        "    model5.add(Dense(len(y_train.shape)))                                          # Output layer\n",
        "    adam = Adam(learning_rate=learning_rate)                                      # Choice of optimizer\n",
        "    # Compiling the model by specifying the choice of loss function ('mae'), and performance metrics\n",
        "    model5.compile(loss='mae', optimizer=adam, metrics=['mae', tfa.metrics.r_square.RSquare()])\n",
        "    return model5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18ffbc9",
      "metadata": {
        "id": "a18ffbc9"
      },
      "outputs": [],
      "source": [
        "# Creating a model with 64 neurons and learning rate 0.001\n",
        "\n",
        "#del model5\n",
        "model5 = create_model(n_neurons=64, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61af1199",
      "metadata": {
        "id": "61af1199"
      },
      "outputs": [],
      "source": [
        "# Presenting the summary of the model\n",
        "\n",
        "model5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38cb4e23",
      "metadata": {
        "id": "38cb4e23"
      },
      "outputs": [],
      "source": [
        "# Training the model for 50 epochs\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "model_history = model5.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854629c8",
      "metadata": {
        "id": "854629c8"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['mae'], label = 'train mae')\n",
        "plt.plot(model_history.history['val_mae'], label = 'test mae')\n",
        "plt.title('History of the Loss Function of the Model')\n",
        "plt.legend(['train mae', 'test mae'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901c3a52",
      "metadata": {
        "id": "901c3a52"
      },
      "outputs": [],
      "source": [
        "# Plotting R^2 score from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['r_square'], label = 'train r^2')\n",
        "plt.plot(model_history.history['val_r_square'], label = 'test r^2')\n",
        "plt.title(r'$R^2$ of the Train and Test Subsets')\n",
        "plt.legend([r'train $R^2$', r'test $R^2$'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e81e2ac1",
      "metadata": {
        "id": "e81e2ac1"
      },
      "source": [
        "# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Fully Connected NN - VI. One Store, One Dept, Relu 128 Neurons</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28667002",
      "metadata": {
        "id": "28667002"
      },
      "outputs": [],
      "source": [
        "features = df2_2.columns[:-1]\n",
        "target = df2_2.columns[-1]\n",
        "\n",
        "X = df2_2[features].values\n",
        "y = df2_2[target].values\n",
        "\n",
        "# Breaking the data into train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b20cd94",
      "metadata": {
        "id": "7b20cd94"
      },
      "outputs": [],
      "source": [
        "# Creating a simple function that builds a fully connected neural network\n",
        "\n",
        "def create_model(n_neurons, learning_rate):\n",
        "    model6 = Sequential()\n",
        "    model6.add(Dense(n_neurons, input_dim=X_train.shape[1], activation='relu')) # Layer 1 + activation\n",
        "    model6.add(Dense(n_neurons, activation='relu'))                             # Layer 2 + activation\n",
        "    model6.add(Dense(n_neurons, activation='relu'))                             # Layer 3 + activation\n",
        "    model6.add(Dense(len(y_train.shape)))                                          # Output layer\n",
        "    adam = Adam(learning_rate=learning_rate)                                      # Choice of optimizer\n",
        "    # Compiling the model by specifying the choice of loss function ('mae'), and performance metrics\n",
        "    model6.compile(loss='mae', optimizer=adam, metrics=['mae', tfa.metrics.r_square.RSquare()])\n",
        "    return model6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee7ba22c",
      "metadata": {
        "id": "ee7ba22c"
      },
      "outputs": [],
      "source": [
        "# Creating a model with 64 neurons and learning rate 0.001\n",
        "\n",
        "#del model6\n",
        "model6 = create_model(n_neurons=64, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8050c620",
      "metadata": {
        "id": "8050c620"
      },
      "outputs": [],
      "source": [
        "# Presenting the summary of the model\n",
        "\n",
        "model6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4498c314",
      "metadata": {
        "id": "4498c314"
      },
      "outputs": [],
      "source": [
        "# Training the model for 25 epochs\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "model_history = model6.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3711a0f5",
      "metadata": {
        "id": "3711a0f5"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss function from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['mae'], label = 'train mae')\n",
        "plt.plot(model_history.history['val_mae'], label = 'test mae')\n",
        "plt.title('History of the Loss Function of the Model')\n",
        "plt.legend(['train mae', 'test mae'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e44b96",
      "metadata": {
        "id": "b1e44b96"
      },
      "outputs": [],
      "source": [
        "# Plotting R^2 score from model's history\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(model_history.history['r_square'], label = 'train r^2')\n",
        "plt.plot(model_history.history['val_r_square'], label = 'test r^2')\n",
        "plt.title(r'$R^2$ of the Train and Test Subsets')\n",
        "plt.legend([r'train $R^2$', r'test $R^2$'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}